{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52bd5ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6173e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ccd58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd5baff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spam_data(ham_url=HAM_URL, spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", ham_url), (\"spam.tar.bz2\", spam_url)):\n",
    "        path= os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file= tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=spam_path)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58434fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d3de3",
   "metadata": {},
   "source": [
    "Load all email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b1ec707",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR= os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR= os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames= [name for name in sorted(os.listdir(HAM_DIR)) if len(name) >20]\n",
    "spam_filenames= [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) >20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d71a1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e09b88af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce83c3",
   "metadata": {},
   "source": [
    "We can use email modules to parse these emails (includes headers, encoding, etc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b6edf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "860d4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory= \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy= email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d52baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails=[load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails=[load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a927fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ca9cc",
   "metadata": {},
   "source": [
    "There are cases where email can have attachments such as images, files or multiparts. Let's explore various email structure that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28f9e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98dd6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def structures_counter(emails):\n",
    "    structures= Counter()\n",
    "    for email in emails:\n",
    "        structure= get_email_structure(email)\n",
    "        structures[structure]+=1\n",
    "    return structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d1ae141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5aea4a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c62b5c",
   "metadata": {},
   "source": [
    "There are more text email in ham emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "683baad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <12a1mailbot1@web.de>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (localhost [127.0.0.1])\tby phobos.labs.spamassassin.taint.org (Postfix) with ESMTP id 136B943C32\tfor <zzzz@localhost>; Thu, 22 Aug 2002 08:17:21 -0400 (EDT)\n",
      "Received : from mail.webnote.net [193.120.211.219]\tby localhost with POP3 (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 13:17:21 +0100 (IST)\n",
      "Received : from dd_it7 ([210.97.77.167])\tby webnote.net (8.9.3/8.9.3) with ESMTP id NAA04623\tfor <zzzz@spamassassin.taint.org>; Thu, 22 Aug 2002 13:09:41 +0100\n",
      "From : 12a1mailbot1@web.de\n",
      "Received : from r-smtp.korea.com - 203.122.2.197 by dd_it7  with Microsoft SMTPSVC(5.5.1775.675.6);\t Sat, 24 Aug 2002 09:42:10 +0900\n",
      "To : dcek1a1@netsgo.com\n",
      "Subject : Life Insurance - Why Pay More?\n",
      "Date : Wed, 21 Aug 2002 20:31:57 -1600\n",
      "MIME-Version : 1.0\n",
      "Message-ID : <0103c1042001882DD_IT7@dd_it7>\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "for header, value in spam_emails[0].items():\n",
    "    print(header,\":\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "30614e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Life Insurance - Why Pay More?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spam_emails[0][\"Subject\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c48df8",
   "metadata": {},
   "source": [
    "Let's divide the train, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "066c0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd985bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array(ham_emails+spam_emails, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4128b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.array([0]*len(ham_emails)+[1]*len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a30fb724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b33cc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85fd3f",
   "metadata": {},
   "source": [
    "Okay, let's start writing the preprocessing functions. First, we will need a function to convert HTML to plain text. Arguably the best way to do this would be to use the great BeautifulSoup library, but I would like to avoid adding another dependency to this project, so let's hack a quick & dirty solution using regular expressions (at the risk of un̨ho͞ly radiańcé destro҉ying all enli̍̈́̂̈́ghtenment). The following function first drops the <head> section, then converts all <a> tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the plain text. For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as '&gt'; or '&nbsp';):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "13cc575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f2efdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a676d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HR>\n",
      "<html>\n",
      "<div bgcolor=\"#FFFFCC\">\n",
      "\n",
      "  <p align=\"center\"><a\n",
      "href=\"http://www.webbasedmailing.com\"><img border=\"0\"\n",
      "src=\"http://www.webbasedmailing.com/Toners2goLogo.jpg\"\n",
      "width=\"349\" height=\"96\"></a></p>\n",
      "<p align=\"center\"><font size=\"6\" face=\"Arial MT\n",
      "Black\"><i>Tremendous Savings</i>\n",
      "on Toners,&nbsp;</font></p>\n",
      "<p align=\"center\"><font size=\"6\" face=\"Arial MT\n",
      "Black\">\n",
      "Inkjets, FAX, and Thermal Replenishables!!</font></p>\n",
      "<p><a href=\"http://www.webbasedmailing.com\">Toners 2 Go\n",
      "</a>is your secret\n",
      "weapon to lowering your cost for <a\n",
      "href=\"http://www.webbasedmailing.com\">High Quality,\n",
      "Low-Cost</a> printer\n",
      "supplies!&nbsp; We have been in the printer\n",
      "replenishables business since 1992,\n",
      "and pride ourselves on rapid response and outstanding\n",
      "customer service.&nbsp;\n",
      "What we sell are 100% compatible replacements for\n",
      "Epson, Canon, Hewlett Packard,\n",
      "Xerox, Okidata, Brother, and Lexmark; products that\n",
      "meet and often exceed\n",
      "original manufacturer's specifications.</p>\n",
      "<p><i><font size=\"4\">Check out these\n",
      "p ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_email= [email for email in X_train[y_train==1]\n",
    "                 if get_email_structure(email)==\"text/html\"]\n",
    "sample_html_spam= html_spam_email[4]\n",
    "print(sample_html_spam.get_content().strip()[:1000], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b79cee",
   "metadata": {},
   "source": [
    "See the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d4e101d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   HYPERLINK\n",
      "Tremendous Savings\n",
      "on Toners, \n",
      "Inkjets, FAX, and Thermal Replenishables!!\n",
      " HYPERLINK Toners 2 Go\n",
      "is your secret\n",
      "weapon to lowering your cost for  HYPERLINK High Quality,\n",
      "Low-Cost printer\n",
      "supplies!  We have been in the printer\n",
      "replenishables business since 1992,\n",
      "and pride ourselves on rapid response and outstanding\n",
      "customer service. \n",
      "What we sell are 100% compatible replacements for\n",
      "Epson, Canon, Hewlett Packard,\n",
      "Xerox, Okidata, Brother, and Lexmark; products that\n",
      "meet and often exceed\n",
      "original manufacturer's specifications.\n",
      "Check out these\n",
      "prices!\n",
      "        Epson Stylus\n",
      "Color inkjet cartridge\n",
      "(SO20108):     Epson's Price:\n",
      "$27.99    \n",
      "Toners2Go price: $9.95!\n",
      "         HP\n",
      "LaserJet 4 Toner Cartridge\n",
      "(92298A):           \n",
      "HP's\n",
      "Price:\n",
      "$88.99           \n",
      "Toners2Go\n",
      "  price: $41.75!\n",
      " \n",
      "Come visit us on the web to check out our hundreds\n",
      "of similar bargains at  HYPERLINK Toners\n",
      "2 Go!\n",
      "  request to be excluded by visiting  HYPERLINK HERE\n",
      "beverley\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79731e12",
   "metadata": {},
   "source": [
    "Let's create a function to take in email as input and output to text form. Whatever the format is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "09db7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html=None\n",
    "    for part in email.walk():\n",
    "        ctype= part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content=part.get_content()\n",
    "        except: #In case of encoding issue\n",
    "            content=str(part.get_payload())\n",
    "        if ctype==\"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html=content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b3f7c9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   HYPERLINK\n",
      "Tremendous Savings\n",
      "on Toners, \n",
      "Inkjets, FAX, and Thermal Replenishables!!\n",
      " HYPERLINK T ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0b2fe",
   "metadata": {},
   "source": [
    "Let's throw in some stemming! For this to work, you need to install the Natural Language Toolkit (NLTK). It's as simple as running the following command (don't forget to activate your virtualenv first; if you don't have one, you will likely need administrator rights, or use the --user option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6174dc8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.8.17-cp310-cp310-win_amd64.whl (263 kB)\n",
      "     -------------------------------------- 263.0/263.0 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.7 regex-2022.8.17 tqdm-4.64.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d64b20d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e2a25",
   "metadata": {},
   "source": [
    "We will also need a way to replace URLs with the word \"URL\". For this, we could use hard core regular expressions but we will just use the urlextract library. You can install it with the following command (don't forget to activate your virtualenv first; if you don't have one, you will likely need administrator rights, or use the --user option):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77bc74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urlextract\n",
      "  Downloading urlextract-1.6.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from urlextract) (3.3)\n",
      "Collecting platformdirs\n",
      "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting uritools\n",
      "  Downloading uritools-4.0.0-py3-none-any.whl (10 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: uritools, platformdirs, filelock, urlextract\n",
      "Successfully installed filelock-3.8.0 platformdirs-2.5.2 uritools-4.0.0 urlextract-1.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e60ac583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['google.com', 'https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urlextract\n",
    "    url_extractor= urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"It will detect google.com and https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454\"))\n",
    "except ImportError:\n",
    "    print(\"Error: Replacing URLS requires for extract value\")\n",
    "    url_extractor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3665ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ab7343de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True, \n",
    "                 replace_urls=True, replace_number=True, stemming=True):\n",
    "        self.strip_headers=strip_headers\n",
    "        self.lower_case=lower_case\n",
    "        self.remove_punctuation= remove_punctuation\n",
    "        self.replace_urls=replace_urls\n",
    "        self.replace_number= replace_number\n",
    "        self.stemming= stemming\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed=[]\n",
    "        for email in X:\n",
    "            text= email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text=text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls= list(url_extractor.find_urls(text))\n",
    "                urls.sort(key= lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text= text.replace(url, \" URL \")\n",
    "            if self.replace_number:\n",
    "                text= re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text= re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts=Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts=Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word= stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word]+=count\n",
    "                word_counts= stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return X_transformed\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5d6ba5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_few= X_train[:3]\n",
    "X_few_wordcounts= EmailToWordCounterTransformer().fit_transform(X_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cb04ac8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
       " Counter({'some': 1,\n",
       "          'interest': 1,\n",
       "          'quot': 1,\n",
       "          'url': 1,\n",
       "          'thoma': 1,\n",
       "          'jefferson': 2,\n",
       "          'i': 2,\n",
       "          'have': 2,\n",
       "          'examin': 1,\n",
       "          'all': 3,\n",
       "          'the': 11,\n",
       "          'known': 1,\n",
       "          'superstit': 2,\n",
       "          'of': 9,\n",
       "          'word': 1,\n",
       "          'and': 8,\n",
       "          'do': 1,\n",
       "          'not': 1,\n",
       "          'find': 1,\n",
       "          'in': 1,\n",
       "          'our': 1,\n",
       "          'particular': 1,\n",
       "          'christian': 3,\n",
       "          'one': 2,\n",
       "          'redeem': 1,\n",
       "          'featur': 1,\n",
       "          'they': 1,\n",
       "          'are': 1,\n",
       "          'alik': 1,\n",
       "          'found': 1,\n",
       "          'on': 2,\n",
       "          'fabl': 1,\n",
       "          'mytholog': 1,\n",
       "          'million': 1,\n",
       "          'innoc': 1,\n",
       "          'men': 1,\n",
       "          'women': 1,\n",
       "          'children': 1,\n",
       "          'sinc': 1,\n",
       "          'introduct': 1,\n",
       "          'been': 2,\n",
       "          'burnt': 1,\n",
       "          'tortur': 1,\n",
       "          'fine': 1,\n",
       "          'imprison': 1,\n",
       "          'what': 1,\n",
       "          'ha': 2,\n",
       "          'effect': 1,\n",
       "          'thi': 1,\n",
       "          'coercion': 1,\n",
       "          'to': 3,\n",
       "          'make': 1,\n",
       "          'half': 2,\n",
       "          'world': 1,\n",
       "          'fool': 1,\n",
       "          'other': 1,\n",
       "          'hypocrit': 1,\n",
       "          'support': 1,\n",
       "          'rogueri': 2,\n",
       "          'error': 1,\n",
       "          'over': 1,\n",
       "          'earth': 1,\n",
       "          'six': 1,\n",
       "          'histor': 1,\n",
       "          'american': 1,\n",
       "          'by': 3,\n",
       "          'john': 1,\n",
       "          'e': 1,\n",
       "          'remsburg': 1,\n",
       "          'letter': 1,\n",
       "          'william': 1,\n",
       "          'short': 1,\n",
       "          'again': 1,\n",
       "          'becom': 1,\n",
       "          'most': 1,\n",
       "          'pervert': 1,\n",
       "          'system': 1,\n",
       "          'that': 1,\n",
       "          'ever': 1,\n",
       "          'shone': 1,\n",
       "          'man': 1,\n",
       "          'absurd': 1,\n",
       "          'untruth': 1,\n",
       "          'were': 1,\n",
       "          'perpetr': 1,\n",
       "          'upon': 1,\n",
       "          'teach': 2,\n",
       "          'jesu': 2,\n",
       "          'a': 1,\n",
       "          'larg': 1,\n",
       "          'band': 1,\n",
       "          'dupe': 1,\n",
       "          'import': 1,\n",
       "          'led': 1,\n",
       "          'paul': 1,\n",
       "          'first': 1,\n",
       "          'great': 1,\n",
       "          'corrupt': 1}),\n",
       " Counter({'in': 2,\n",
       "          'forteana': 2,\n",
       "          'y': 1,\n",
       "          'martin': 2,\n",
       "          'adamson': 1,\n",
       "          's': 3,\n",
       "          'wrote': 1,\n",
       "          'for': 1,\n",
       "          'an': 2,\n",
       "          'altern': 1,\n",
       "          'and': 2,\n",
       "          'rather': 1,\n",
       "          'more': 1,\n",
       "          'factual': 1,\n",
       "          'base': 1,\n",
       "          'rundown': 1,\n",
       "          'on': 1,\n",
       "          'hamza': 1,\n",
       "          'career': 1,\n",
       "          'includ': 1,\n",
       "          'hi': 1,\n",
       "          'belief': 1,\n",
       "          'that': 1,\n",
       "          'all': 1,\n",
       "          'non': 1,\n",
       "          'muslim': 1,\n",
       "          'yemen': 1,\n",
       "          'should': 1,\n",
       "          'be': 1,\n",
       "          'murder': 1,\n",
       "          'outright': 1,\n",
       "          'url': 4,\n",
       "          'we': 2,\n",
       "          'know': 1,\n",
       "          'how': 1,\n",
       "          'unbias': 1,\n",
       "          'memri': 1,\n",
       "          'is': 2,\n",
       "          'don': 1,\n",
       "          't': 1,\n",
       "          'html': 1,\n",
       "          'rob': 1,\n",
       "          'yahoo': 2,\n",
       "          'group': 3,\n",
       "          'sponsor': 1,\n",
       "          'number': 1,\n",
       "          'dvd': 1,\n",
       "          'free': 1,\n",
       "          'p': 1,\n",
       "          'join': 1,\n",
       "          'now': 1,\n",
       "          'to': 3,\n",
       "          'unsubscrib': 2,\n",
       "          'from': 1,\n",
       "          'thi': 1,\n",
       "          'send': 1,\n",
       "          'email': 1,\n",
       "          'egroup': 1,\n",
       "          'com': 1,\n",
       "          'your': 1,\n",
       "          'use': 1,\n",
       "          'of': 1,\n",
       "          'subject': 1})]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f77b01",
   "metadata": {},
   "source": [
    "With converting email into words, now we can vectorize the entire email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9343e8f",
   "metadata": {},
   "source": [
    "For this, we will have to create a transformer whose fit() method will build the vocabulary and transform() method to convert word counts into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1b9c0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d6f88425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size=vocabulary_size\n",
    "    def fit(self,X,y=None):\n",
    "        total_count=Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word]+= min(count,10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_= {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows=[]\n",
    "        cols=[]\n",
    "        data=[]\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word,0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows,cols)), shape=(len(X), self.vocabulary_size+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9f8518a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 20 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer= WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors= vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0f3f1914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [99, 11,  9,  8,  3,  1,  3,  1,  3,  2,  3],\n",
       "       [67,  0,  1,  2,  3,  4,  1,  2,  0,  1,  0]], dtype=int32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100aeb42",
   "metadata": {},
   "source": [
    "In a nutshell, for the second row. There are 99 words that are not part of the dictionary. The 11 means the first words existed 11 times in the email, second word appears 9 times, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6fe93a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'and': 3,\n",
       " 'to': 4,\n",
       " 'url': 5,\n",
       " 'all': 6,\n",
       " 'in': 7,\n",
       " 'christian': 8,\n",
       " 'on': 9,\n",
       " 'by': 10}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d37f3",
   "metadata": {},
   "source": [
    "We are now ready to train our first spam classifier! Let's transform the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c2314e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3ad6dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline= Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer())\n",
    "])\n",
    "\n",
    "X_train_transformed= preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d70691",
   "metadata": {},
   "source": [
    "Now we can begin training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3d571b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fc701d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.0s\n",
      "[CV] END .................................................... total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "log_clf= LogisticRegression(random_state=42, solver='lbfgs', max_iter=1000)\n",
    "score= cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "428f4947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a23df3",
   "metadata": {},
   "source": [
    "98% is not a number to scoff at. However, this dataset is really small (3000), so we might not be able to get so lucky with a larger dataset. However, the procedure are the same:\n",
    "- Try other model and find the best base model\n",
    "- Hyperparam tuning with GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b04a4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.88%\n",
      "Recall: 97.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469d1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
